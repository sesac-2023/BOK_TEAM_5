{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "\n",
    "target_date = \"2015-07\" #예측 날짜\n",
    "seed_word_dict = {\"hawkish\": ['높', '팽창', '인상', '매파', '성장', '투기_억제', '상승',  '인플레이션_압력', '증가', '위험_선호', '상회', '물가_상승', '과열', '금리_상승', '확장', '상방_압력', '긴축', '변동성_감소', '흑자', '채권_가격_하락', '견조', '요금_인상', '낙관', '부동산_가격_상승', '상향']\n",
    "                \n",
    "        \n",
    "                ,\"doveish\":[\"낯\", \"축\", \"인하\", \"비둘기\", \"둔화\", \"악화\", \"하락\", \"회복_못하\", \"감소\", \"위험_회피\", \"하회\", \"물가_하락\", \"위축\", \"금리_하락\", \"침체\", \"하방_압력\", \"완화\", \"변동성_확대\", \"적자\", \"채권_가격_상승\", \"부진\", \"요금_인하\", \"비관\", \"부동산_가격_하락\", \"하향\"]}\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"./train_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "      <th>1_ngrams</th>\n",
       "      <th>2_ngrams</th>\n",
       "      <th>3_ngrams</th>\n",
       "      <th>4_ngrams</th>\n",
       "      <th>5_ngrams</th>\n",
       "      <th>new_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>월 기점 채권 금리 하락세 변곡점 형성 전망 요약 결론 추가 금리 인하 불확실성 경...</td>\n",
       "      <td>doveish</td>\n",
       "      <td>월 기점 채권 금리 하락세 변곡점 형성 전망 요약 결론 추가 금리 인하 불확실성 경...</td>\n",
       "      <td>월_기점 기점_채권 채권_금리 금리_하락세 하락세_변곡점 변곡점_형성 형성_전망 전...</td>\n",
       "      <td>월_기점_채권 기점_채권_금리 채권_금리_하락세 금리_하락세_변곡점 하락세_변곡점_...</td>\n",
       "      <td>월_기점_채권_금리 기점_채권_금리_하락세 채권_금리_하락세_변곡점 금리_하락세_변...</td>\n",
       "      <td>월_기점_채권_금리_하락세 기점_채권_금리_하락세_변곡점 채권_금리_하락세_변곡점_...</td>\n",
       "      <td>월 기점 채권 금리 하락세 변곡점 형성 전망 요약 결론 추가 금리 인하 불확실성 경...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>일 금리 인하 요약 월기 금리 인하 이후 도금 통 위 기대감 지속 월금 통 위 글로...</td>\n",
       "      <td>doveish</td>\n",
       "      <td>일 금리 인하 요약 월기 금리 인하 이후 도금 통 위 기대감 지속 월금 통 위 글로...</td>\n",
       "      <td>일_금리 금리_인하 인하_요약 요약_월기 월기_금리 금리_인하 인하_이후 이후_도금...</td>\n",
       "      <td>일_금리_인하 금리_인하_요약 인하_요약_월기 요약_월기_금리 월기_금리_인하 금리...</td>\n",
       "      <td>일_금리_인하_요약 금리_인하_요약_월기 인하_요약_월기_금리 요약_월기_금리_인하...</td>\n",
       "      <td>일_금리_인하_요약_월기 금리_인하_요약_월기_금리 인하_요약_월기_금리_인하 요약...</td>\n",
       "      <td>일 금리 인하 요약 월기 금리 인하 이후 도금 통 위 기대감 지속 월금 통 위 글로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>월금 통 위 이후 향후 금리 방향 점검 요약 지난주 기준금리 인하 결정 채권시장 약...</td>\n",
       "      <td>doveish</td>\n",
       "      <td>월금 통 위 이후 향후 금리 방향 점검 요약 지난주 기준금리 인하 결정 채권시장 약...</td>\n",
       "      <td>월금_통 통_위 위_이후 이후_향후 향후_금리 금리_방향 방향_점검 점검_요약 요약...</td>\n",
       "      <td>월금_통_위 통_위_이후 위_이후_향후 이후_향후_금리 향후_금리_방향 금리_방향_...</td>\n",
       "      <td>월금_통_위_이후 통_위_이후_향후 위_이후_향후_금리 이후_향후_금리_방향 향후_...</td>\n",
       "      <td>월금_통_위_이후_향후 통_위_이후_향후_금리 위_이후_향후_금리_방향 이후_향후_...</td>\n",
       "      <td>월금 통 위 이후 향후 금리 방향 점검 요약 지난주 기준금리 인하 결정 채권시장 약...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>강세 준비 숨고르기 요약 강세 준비 숨고르기 금주 채권 금리 월금 통 위 이후 추가...</td>\n",
       "      <td>doveish</td>\n",
       "      <td>강세 준비 숨고르기 요약 강세 준비 숨고르기 금주 채권 금리 월금 통 위 이후 추가...</td>\n",
       "      <td>강세_준비 준비_숨고르기 숨고르기_요약 요약_강세 강세_준비 준비_숨고르기 숨고르기...</td>\n",
       "      <td>강세_준비_숨고르기 준비_숨고르기_요약 숨고르기_요약_강세 요약_강세_준비 강세_준...</td>\n",
       "      <td>강세_준비_숨고르기_요약 준비_숨고르기_요약_강세 숨고르기_요약_강세_준비 요약_강...</td>\n",
       "      <td>강세_준비_숨고르기_요약_강세 준비_숨고르기_요약_강세_준비 숨고르기_요약_강세_준...</td>\n",
       "      <td>강세 준비 숨고르기 요약 강세 준비 숨고르기 금주 채권 금리 월금 통 위 이후 추가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미국 금리 급등 스캔들 요약 이번 주채 시장 전망 약화 정책 기대 미국 금리 급등 ...</td>\n",
       "      <td>doveish</td>\n",
       "      <td>미국 금리 급등 스캔들 요약 이번 주채 시장 전망 약화 정책 기대 미국 금리 급등 ...</td>\n",
       "      <td>미국_금리 금리_급등 급등_스캔들 스캔들_요약 요약_이번 이번_주채 주채_시장 시장...</td>\n",
       "      <td>미국_금리_급등 금리_급등_스캔들 급등_스캔들_요약 스캔들_요약_이번 요약_이번_주...</td>\n",
       "      <td>미국_금리_급등_스캔들 금리_급등_스캔들_요약 급등_스캔들_요약_이번 스캔들_요약_...</td>\n",
       "      <td>미국_금리_급등_스캔들_요약 금리_급등_스캔들_요약_이번 급등_스캔들_요약_이번_주...</td>\n",
       "      <td>미국 금리 급등 스캔들 요약 이번 주채 시장 전망 약화 정책 기대 미국 금리 급등 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154441</th>\n",
       "      <td>표 행간 단기 금리 기준 코리보 일 다음 일 전시 고시 한국 은행 단기 금리 기준 ...</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>표 행간 단기 금리 기준 코리보 일 다음 일 전시 고시 한국 은행 단기 금리 기준 ...</td>\n",
       "      <td>표_행간 행간_단기 단기_금리 금리_기준 기준_코리보 코리보_일 일_다음 다음_일 ...</td>\n",
       "      <td>표_행간_단기 행간_단기_금리 단기_금리_기준 금리_기준_코리보 기준_코리보_일 코...</td>\n",
       "      <td>표_행간_단기_금리 행간_단기_금리_기준 단기_금리_기준_코리보 금리_기준_코리보_...</td>\n",
       "      <td>표_행간_단기_금리_기준 행간_단기_금리_기준_코리보 단기_금리_기준_코리보_일 금...</td>\n",
       "      <td>표 행간 단기 금리 기준 코리보 일 다음 일 전시 고시 한국 은행 단기 금리 기준 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154442</th>\n",
       "      <td>채권 오전 금리 상승국 고년 입찰 여파 기자 국고채 금리 오전 중 상승 채권시장 따...</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>채권 오전 금리 상승국 고년 입찰 여파 기자 국고채 금리 오전 중 상승 채권시장 따...</td>\n",
       "      <td>채권_오전 오전_금리 금리_상승국 상승국_고년 고년_입찰 입찰_여파 여파_기자 기자...</td>\n",
       "      <td>채권_오전_금리 오전_금리_상승국 금리_상승국_고년 상승국_고년_입찰 고년_입찰_여...</td>\n",
       "      <td>채권_오전_금리_상승국 오전_금리_상승국_고년 금리_상승국_고년_입찰 상승국_고년_...</td>\n",
       "      <td>채권_오전_금리_상승국_고년 오전_금리_상승국_고년_입찰 금리_상승국_고년_입찰_여...</td>\n",
       "      <td>채권 오전 금리 상승국 고년 입찰 여파 기자 국고채 금리 오전 중 상승 채권시장 따...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154443</th>\n",
       "      <td>국채선물 낙폭 확대 입찰 소화 기자 국채선물 오후 낙폭 확대 채권시장 따르 국채선물...</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>국채선물 낙폭 확대 입찰 소화 기자 국채선물 오후 낙폭 확대 채권시장 따르 국채선물...</td>\n",
       "      <td>국채선물_낙폭 낙폭_확대 확대_입찰 입찰_소화 소화_기자 기자_국채선물 국채선물_오...</td>\n",
       "      <td>국채선물_낙폭_확대 낙폭_확대_입찰 확대_입찰_소화 입찰_소화_기자 소화_기자_국채...</td>\n",
       "      <td>국채선물_낙폭_확대_입찰 낙폭_확대_입찰_소화 확대_입찰_소화_기자 입찰_소화_기자...</td>\n",
       "      <td>국채선물_낙폭_확대_입찰_소화 낙폭_확대_입찰_소화_기자 확대_입찰_소화_기자_국채...</td>\n",
       "      <td>국채선물 낙폭 확대 입찰 소화 기자 국채선물 오후 낙폭 확대 채권시장 따르 국채선물...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154444</th>\n",
       "      <td>금리 상승 국채선물 하락 은행권 비드 기자 금리스와프 금리 상승 마감 국채선물 하락...</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>금리 상승 국채선물 하락 은행권 비드 기자 금리스와프 금리 상승 마감 국채선물 하락...</td>\n",
       "      <td>금리_상승 상승_국채선물 국채선물_하락 하락_은행권 은행권_비드 비드_기자 기자_금...</td>\n",
       "      <td>금리_상승_국채선물 상승_국채선물_하락 국채선물_하락_은행권 하락_은행권_비드 은행...</td>\n",
       "      <td>금리_상승_국채선물_하락 상승_국채선물_하락_은행권 국채선물_하락_은행권_비드 하락...</td>\n",
       "      <td>금리_상승_국채선물_하락_은행권 상승_국채선물_하락_은행권_비드 국채선물_하락_은행...</td>\n",
       "      <td>금리 상승 국채선물 하락 은행권 비드 기자 금리스와프 금리 상승 마감 국채선물 하락...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154445</th>\n",
       "      <td>달러화 제한적 강세 엔화 국채 임시 매입 약세 뉴욕 특파원 달러화가치 제한적 강세 ...</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>달러화 제한적 강세 엔화 국채 임시 매입 약세 뉴욕 특파원 달러화가치 제한적 강세 ...</td>\n",
       "      <td>달러화_제한적 제한적_강세 강세_엔화 엔화_국채 국채_임시 임시_매입 매입_약세 약...</td>\n",
       "      <td>달러화_제한적_강세 제한적_강세_엔화 강세_엔화_국채 엔화_국채_임시 국채_임시_매...</td>\n",
       "      <td>달러화_제한적_강세_엔화 제한적_강세_엔화_국채 강세_엔화_국채_임시 엔화_국채_임...</td>\n",
       "      <td>달러화_제한적_강세_엔화_국채 제한적_강세_엔화_국채_임시 강세_엔화_국채_임시_매...</td>\n",
       "      <td>달러화 제한적 강세 엔화 국채 임시 매입 약세 뉴욕 특파원 달러화가치 제한적 강세 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154446 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    words    label  \\\n",
       "0       월 기점 채권 금리 하락세 변곡점 형성 전망 요약 결론 추가 금리 인하 불확실성 경...  doveish   \n",
       "1       일 금리 인하 요약 월기 금리 인하 이후 도금 통 위 기대감 지속 월금 통 위 글로...  doveish   \n",
       "2       월금 통 위 이후 향후 금리 방향 점검 요약 지난주 기준금리 인하 결정 채권시장 약...  doveish   \n",
       "3       강세 준비 숨고르기 요약 강세 준비 숨고르기 금주 채권 금리 월금 통 위 이후 추가...  doveish   \n",
       "4       미국 금리 급등 스캔들 요약 이번 주채 시장 전망 약화 정책 기대 미국 금리 급등 ...  doveish   \n",
       "...                                                   ...      ...   \n",
       "154441  표 행간 단기 금리 기준 코리보 일 다음 일 전시 고시 한국 은행 단기 금리 기준 ...  hawkish   \n",
       "154442  채권 오전 금리 상승국 고년 입찰 여파 기자 국고채 금리 오전 중 상승 채권시장 따...  hawkish   \n",
       "154443  국채선물 낙폭 확대 입찰 소화 기자 국채선물 오후 낙폭 확대 채권시장 따르 국채선물...  hawkish   \n",
       "154444  금리 상승 국채선물 하락 은행권 비드 기자 금리스와프 금리 상승 마감 국채선물 하락...  hawkish   \n",
       "154445  달러화 제한적 강세 엔화 국채 임시 매입 약세 뉴욕 특파원 달러화가치 제한적 강세 ...  hawkish   \n",
       "\n",
       "                                                 1_ngrams  \\\n",
       "0       월 기점 채권 금리 하락세 변곡점 형성 전망 요약 결론 추가 금리 인하 불확실성 경...   \n",
       "1       일 금리 인하 요약 월기 금리 인하 이후 도금 통 위 기대감 지속 월금 통 위 글로...   \n",
       "2       월금 통 위 이후 향후 금리 방향 점검 요약 지난주 기준금리 인하 결정 채권시장 약...   \n",
       "3       강세 준비 숨고르기 요약 강세 준비 숨고르기 금주 채권 금리 월금 통 위 이후 추가...   \n",
       "4       미국 금리 급등 스캔들 요약 이번 주채 시장 전망 약화 정책 기대 미국 금리 급등 ...   \n",
       "...                                                   ...   \n",
       "154441  표 행간 단기 금리 기준 코리보 일 다음 일 전시 고시 한국 은행 단기 금리 기준 ...   \n",
       "154442  채권 오전 금리 상승국 고년 입찰 여파 기자 국고채 금리 오전 중 상승 채권시장 따...   \n",
       "154443  국채선물 낙폭 확대 입찰 소화 기자 국채선물 오후 낙폭 확대 채권시장 따르 국채선물...   \n",
       "154444  금리 상승 국채선물 하락 은행권 비드 기자 금리스와프 금리 상승 마감 국채선물 하락...   \n",
       "154445  달러화 제한적 강세 엔화 국채 임시 매입 약세 뉴욕 특파원 달러화가치 제한적 강세 ...   \n",
       "\n",
       "                                                 2_ngrams  \\\n",
       "0       월_기점 기점_채권 채권_금리 금리_하락세 하락세_변곡점 변곡점_형성 형성_전망 전...   \n",
       "1       일_금리 금리_인하 인하_요약 요약_월기 월기_금리 금리_인하 인하_이후 이후_도금...   \n",
       "2       월금_통 통_위 위_이후 이후_향후 향후_금리 금리_방향 방향_점검 점검_요약 요약...   \n",
       "3       강세_준비 준비_숨고르기 숨고르기_요약 요약_강세 강세_준비 준비_숨고르기 숨고르기...   \n",
       "4       미국_금리 금리_급등 급등_스캔들 스캔들_요약 요약_이번 이번_주채 주채_시장 시장...   \n",
       "...                                                   ...   \n",
       "154441  표_행간 행간_단기 단기_금리 금리_기준 기준_코리보 코리보_일 일_다음 다음_일 ...   \n",
       "154442  채권_오전 오전_금리 금리_상승국 상승국_고년 고년_입찰 입찰_여파 여파_기자 기자...   \n",
       "154443  국채선물_낙폭 낙폭_확대 확대_입찰 입찰_소화 소화_기자 기자_국채선물 국채선물_오...   \n",
       "154444  금리_상승 상승_국채선물 국채선물_하락 하락_은행권 은행권_비드 비드_기자 기자_금...   \n",
       "154445  달러화_제한적 제한적_강세 강세_엔화 엔화_국채 국채_임시 임시_매입 매입_약세 약...   \n",
       "\n",
       "                                                 3_ngrams  \\\n",
       "0       월_기점_채권 기점_채권_금리 채권_금리_하락세 금리_하락세_변곡점 하락세_변곡점_...   \n",
       "1       일_금리_인하 금리_인하_요약 인하_요약_월기 요약_월기_금리 월기_금리_인하 금리...   \n",
       "2       월금_통_위 통_위_이후 위_이후_향후 이후_향후_금리 향후_금리_방향 금리_방향_...   \n",
       "3       강세_준비_숨고르기 준비_숨고르기_요약 숨고르기_요약_강세 요약_강세_준비 강세_준...   \n",
       "4       미국_금리_급등 금리_급등_스캔들 급등_스캔들_요약 스캔들_요약_이번 요약_이번_주...   \n",
       "...                                                   ...   \n",
       "154441  표_행간_단기 행간_단기_금리 단기_금리_기준 금리_기준_코리보 기준_코리보_일 코...   \n",
       "154442  채권_오전_금리 오전_금리_상승국 금리_상승국_고년 상승국_고년_입찰 고년_입찰_여...   \n",
       "154443  국채선물_낙폭_확대 낙폭_확대_입찰 확대_입찰_소화 입찰_소화_기자 소화_기자_국채...   \n",
       "154444  금리_상승_국채선물 상승_국채선물_하락 국채선물_하락_은행권 하락_은행권_비드 은행...   \n",
       "154445  달러화_제한적_강세 제한적_강세_엔화 강세_엔화_국채 엔화_국채_임시 국채_임시_매...   \n",
       "\n",
       "                                                 4_ngrams  \\\n",
       "0       월_기점_채권_금리 기점_채권_금리_하락세 채권_금리_하락세_변곡점 금리_하락세_변...   \n",
       "1       일_금리_인하_요약 금리_인하_요약_월기 인하_요약_월기_금리 요약_월기_금리_인하...   \n",
       "2       월금_통_위_이후 통_위_이후_향후 위_이후_향후_금리 이후_향후_금리_방향 향후_...   \n",
       "3       강세_준비_숨고르기_요약 준비_숨고르기_요약_강세 숨고르기_요약_강세_준비 요약_강...   \n",
       "4       미국_금리_급등_스캔들 금리_급등_스캔들_요약 급등_스캔들_요약_이번 스캔들_요약_...   \n",
       "...                                                   ...   \n",
       "154441  표_행간_단기_금리 행간_단기_금리_기준 단기_금리_기준_코리보 금리_기준_코리보_...   \n",
       "154442  채권_오전_금리_상승국 오전_금리_상승국_고년 금리_상승국_고년_입찰 상승국_고년_...   \n",
       "154443  국채선물_낙폭_확대_입찰 낙폭_확대_입찰_소화 확대_입찰_소화_기자 입찰_소화_기자...   \n",
       "154444  금리_상승_국채선물_하락 상승_국채선물_하락_은행권 국채선물_하락_은행권_비드 하락...   \n",
       "154445  달러화_제한적_강세_엔화 제한적_강세_엔화_국채 강세_엔화_국채_임시 엔화_국채_임...   \n",
       "\n",
       "                                                 5_ngrams  \\\n",
       "0       월_기점_채권_금리_하락세 기점_채권_금리_하락세_변곡점 채권_금리_하락세_변곡점_...   \n",
       "1       일_금리_인하_요약_월기 금리_인하_요약_월기_금리 인하_요약_월기_금리_인하 요약...   \n",
       "2       월금_통_위_이후_향후 통_위_이후_향후_금리 위_이후_향후_금리_방향 이후_향후_...   \n",
       "3       강세_준비_숨고르기_요약_강세 준비_숨고르기_요약_강세_준비 숨고르기_요약_강세_준...   \n",
       "4       미국_금리_급등_스캔들_요약 금리_급등_스캔들_요약_이번 급등_스캔들_요약_이번_주...   \n",
       "...                                                   ...   \n",
       "154441  표_행간_단기_금리_기준 행간_단기_금리_기준_코리보 단기_금리_기준_코리보_일 금...   \n",
       "154442  채권_오전_금리_상승국_고년 오전_금리_상승국_고년_입찰 금리_상승국_고년_입찰_여...   \n",
       "154443  국채선물_낙폭_확대_입찰_소화 낙폭_확대_입찰_소화_기자 확대_입찰_소화_기자_국채...   \n",
       "154444  금리_상승_국채선물_하락_은행권 상승_국채선물_하락_은행권_비드 국채선물_하락_은행...   \n",
       "154445  달러화_제한적_강세_엔화_국채 제한적_강세_엔화_국채_임시 강세_엔화_국채_임시_매...   \n",
       "\n",
       "                                               new_column  \n",
       "0       월 기점 채권 금리 하락세 변곡점 형성 전망 요약 결론 추가 금리 인하 불확실성 경...  \n",
       "1       일 금리 인하 요약 월기 금리 인하 이후 도금 통 위 기대감 지속 월금 통 위 글로...  \n",
       "2       월금 통 위 이후 향후 금리 방향 점검 요약 지난주 기준금리 인하 결정 채권시장 약...  \n",
       "3       강세 준비 숨고르기 요약 강세 준비 숨고르기 금주 채권 금리 월금 통 위 이후 추가...  \n",
       "4       미국 금리 급등 스캔들 요약 이번 주채 시장 전망 약화 정책 기대 미국 금리 급등 ...  \n",
       "...                                                   ...  \n",
       "154441  표 행간 단기 금리 기준 코리보 일 다음 일 전시 고시 한국 은행 단기 금리 기준 ...  \n",
       "154442  채권 오전 금리 상승국 고년 입찰 여파 기자 국고채 금리 오전 중 상승 채권시장 따...  \n",
       "154443  국채선물 낙폭 확대 입찰 소화 기자 국채선물 오후 낙폭 확대 채권시장 따르 국채선물...  \n",
       "154444  금리 상승 국채선물 하락 은행권 비드 기자 금리스와프 금리 상승 마감 국채선물 하락...  \n",
       "154445  달러화 제한적 강세 엔화 국채 임시 매입 약세 뉴욕 특파원 달러화가치 제한적 강세 ...  \n",
       "\n",
       "[154446 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = train_df['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_list\n",
    "\n",
    "# 2. 특성 추출 (형태소 분석 및 N-그램)\n",
    "vocab = [token.split() for token in X_train]\n",
    "\n",
    "# 3. N-그램 추출\n",
    "phrases = Phrases([x for x in vocab], min_count=1, threshold=1.1)\n",
    "ngram_X_train = [' '.join(phrases[x]) for x in vocab]\n",
    "New_test_ngrams = ngram_X_train\n",
    "\n",
    "# 4. Ngram2Vec 모델 학습\n",
    "model = Word2Vec([x.split() for x in New_test_ngrams], vector_size=300, window=5, min_count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 생성 하였습니다\n",
      "극성 기준 사전 생성 하였습니다\n",
      "학습 단어 극성 분류 사전 생성 하였습니다\n",
      "새로운 문서에 대한 스코어 계산을 시작합니다\n",
      "target_text 생성\n",
      "634 1253\n",
      "2015-07 해당 날짜는 매파적입니다\n"
     ]
    }
   ],
   "source": [
    "# 이걸로 테스트\n",
    "\n",
    "# 넘파이 코사인 유사도 계산\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def ngram2vec_train(train_list):\n",
    "    X_train = train_list\n",
    "\n",
    "    # 2. 특성 추출 (형태소 분석 및 N-그램)\n",
    "    vocab = [token.split() for token in X_train]\n",
    "\n",
    "    # 3. N-그램 추출\n",
    "    phrases = Phrases([x for x in vocab], min_count=1, threshold=1.1)\n",
    "    ngram_X_train = [' '.join(phrases[x]) for x in vocab]\n",
    "    New_test_ngrams = ngram_X_train\n",
    "\n",
    "    # 4. Ngram2Vec 모델 학습\n",
    "    model = Word2Vec([x.split() for x in New_test_ngrams], vector_size=300, window=5, min_count=25)\n",
    "    return model,phrases\n",
    "model,phrases = ngram2vec_train(train_list)\n",
    "print(\"모델 생성 하였습니다\")\n",
    "\n",
    "# 극성 기준 사전\n",
    "doveish_word_list = []\n",
    "hawkish_word_list = []\n",
    "for key in seed_word_dict['doveish']:\n",
    "    try:\n",
    "        target = model.wv[key]\n",
    "        doveish_word_list.append(key)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for key in seed_word_dict['hawkish']:\n",
    "    try:\n",
    "        target = model.wv[key]\n",
    "        hawkish_word_list.append(key)\n",
    "    except:\n",
    "        pass\n",
    "model_keys = model.wv.index_to_key\n",
    "print(\"극성 기준 사전 생성 하였습니다\")\n",
    "\n",
    "# 카운팅할 극성 워드 생성\n",
    "Word_Emotion_dict = {\"doveish\":[],\"hawkish\":[]}\n",
    "for key in model_keys:\n",
    "    doveish_score = 0\n",
    "    hawkish_score = 0\n",
    "    target = model.wv[key]\n",
    "    for word in doveish_word_list:\n",
    "        doveish_target = model.wv[word]\n",
    "        doveish_score += cos_sim(target, doveish_target)\n",
    "    for word in hawkish_word_list:\n",
    "        hawkish_target = model.wv[word]\n",
    "        hawkish_score += cos_sim(target, hawkish_target)\n",
    "    if doveish_score / len(doveish_word_list) > hawkish_score / len(hawkish_word_list):\n",
    "        #print(f\"{key} 해당 키는 비둘기적입니다\")\n",
    "        Word_Emotion_dict[\"doveish\"].append(key)\n",
    "    elif doveish_score / len(doveish_word_list) == hawkish_score / len(hawkish_word_list):\n",
    "        print(f\"{key} 해당 키는 중립적입니다\")\n",
    "    elif doveish_score / len(doveish_word_list) < hawkish_score / len(hawkish_word_list):\n",
    "        #print(f\"{key} 해당 키는 매파적입니다\")\n",
    "        Word_Emotion_dict[\"hawkish\"].append(key)\n",
    "\n",
    "print(\"학습 단어 극성 분류 사전 생성 하였습니다\")\n",
    "print(\"새로운 문서에 대한 스코어 계산을 시작합니다\")\n",
    "\n",
    "start_date = \"2013-05-09\"\n",
    "def select_MPB(target_date):\n",
    "    def date_range(start, end):\n",
    "        start = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "        end = datetime.strptime(end, \"%Y-%m\")\n",
    "        before_one_day = end - relativedelta(days=30)\n",
    "        dates = [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((before_one_day-start).days+1)]\n",
    "        return dates\n",
    "    dates = date_range(start_date, target_date)\n",
    "\n",
    "    path = \"./MPB_crawling_dataset\"\n",
    "    file_list = []\n",
    "    if os.path.isdir(path):\n",
    "        text_list = os.listdir(path)\n",
    "        for i in text_list:\n",
    "            if i[-3:] == \"txt\":\n",
    "                target_file = i[:-4]\n",
    "                if target_file in dates:\n",
    "                    file_list.append(i)\n",
    "    MPB_file = file_list[-1]                \n",
    "    f = open(f\"{path}/{MPB_file}\", 'r')\n",
    "    file_text = f.readline()\n",
    "    return file_text\n",
    "\n",
    "file_text = select_MPB(target_date)\n",
    "target_text = (phrases[file_text.split()])\n",
    "print(\"target_text 생성\")\n",
    "\n",
    "# 새로운 문서에 대한 스코어 계산\n",
    "new_word_doveish_score = 0\n",
    "new_word_hawkish_score = 0\n",
    "for token in target_text:\n",
    "    if token in Word_Emotion_dict['doveish']:\n",
    "        new_word_doveish_score += 1 \n",
    "    elif token in Word_Emotion_dict['hawkish']:\n",
    "        new_word_hawkish_score += 1\n",
    "print(new_word_doveish_score, new_word_hawkish_score)\n",
    "if new_word_doveish_score >  new_word_hawkish_score:\n",
    "    print(f\"{target_date} 해당 날짜는 비둘기적입니다\")\n",
    "    #return f\"{target_date} 해당 날짜는 비둘기적입니다\"\n",
    "elif new_word_doveish_score ==  new_word_hawkish_score:\n",
    "    print(\"{target_date} 해당 날짜는 중립 성향 입니다\")\n",
    "    #return f\"{target_date} 해당 날짜는 중립 성향 입니다\"\n",
    "elif new_word_doveish_score <  new_word_hawkish_score:\n",
    "    print(f\"{target_date} 해당 날짜는 매파적입니다\")\n",
    "    #return f\"{target_date} 해당 날짜는 매파적입니다\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#최종 함수\n",
    "def model_start(target_date,data_df): \n",
    "    start_date = \"2013-05-09\"\n",
    "    def select_MPB(target_date):\n",
    "        def date_range(start, end):\n",
    "            start = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "            end = datetime.strptime(end, \"%Y-%m\")\n",
    "            before_one_day = end - relativedelta(days=30)\n",
    "            dates = [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((before_one_day-start).days+1)]\n",
    "            return dates\n",
    "        dates = date_range(start_date, target_date)\n",
    "\n",
    "        path = \"./MPB_crawling_dataset\"\n",
    "        file_list = []\n",
    "        if os.path.isdir(path):\n",
    "            text_list = os.listdir(path)\n",
    "            for i in text_list:\n",
    "                if i[-3:] == \"txt\":\n",
    "                    target_file = i[:-4]\n",
    "                    if target_file in dates:\n",
    "                        file_list.append(i)\n",
    "        MPB_file = file_list[-1]                \n",
    "        f = open(f\"{path}/{MPB_file}\", 'r')\n",
    "        file_text = f.readline()\n",
    "        return file_text\n",
    "\n",
    "    file_text = select_MPB(target_date)\n",
    "    target_text = (phrases[file_text.split()])\n",
    "    print(\"target_text 생성\")\n",
    "\n",
    "\n",
    "    # 넘파이 코사인 유사도 계산\n",
    "    def cos_sim(A, B):\n",
    "        return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "    def ngram2vec_train(data_df):\n",
    "        X_train = data_df\n",
    "\n",
    "        # 2. 특성 추출 (형태소 분석 및 N-그램)\n",
    "        vocab = [token.split() for token in X_train]\n",
    "\n",
    "        # 3. N-그램 추출\n",
    "        phrases = Phrases([x for x in vocab], min_count=1, threshold=1.1)\n",
    "        ngram_X_train = [' '.join(phrases[x]) for x in vocab]\n",
    "        New_test_ngrams = ngram_X_train\n",
    "\n",
    "        # 4. Ngram2Vec 모델 학습\n",
    "        model = Word2Vec([x.split() for x in New_test_ngrams], vector_size=300, window=5, min_count=25)\n",
    "        return model\n",
    "    model = ngram2vec_train(data_df)\n",
    "    print(\"모델 생성 하였습니다\")\n",
    "\n",
    "    # 극성 기준 사전\n",
    "    doveish_word_list = []\n",
    "    hawkish_word_list = []\n",
    "    for key in seed_word_dict['doveish']:\n",
    "        try:\n",
    "            target = model.wv[key]\n",
    "            doveish_word_list.append(key)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for key in seed_word_dict['hawkish']:\n",
    "        try:\n",
    "            target = model.wv[key]\n",
    "            hawkish_word_list.append(key)\n",
    "        except:\n",
    "            pass\n",
    "    model_keys = model.wv.index_to_key\n",
    "    print(\"극성 기준 사전 생성 하였습니다\")\n",
    "\n",
    "    # 카운팅할 극성 워드 생성\n",
    "    Word_Emotion_dict = {\"doveish\":[],\"hawkish\":[]}\n",
    "    for key in model_keys:\n",
    "        doveish_score = 0\n",
    "        hawkish_score = 0\n",
    "        target = model.wv[key]\n",
    "        for word in doveish_word_list:\n",
    "            doveish_target = model.wv[word]\n",
    "            doveish_score += cos_sim(target, doveish_target)\n",
    "        for word in hawkish_word_list:\n",
    "            hawkish_target = model.wv[word]\n",
    "            hawkish_score += cos_sim(target, hawkish_target)\n",
    "        if doveish_score / len(doveish_word_list) > hawkish_score / len(hawkish_word_list):\n",
    "            #print(f\"{key} 해당 키는 비둘기적입니다\")\n",
    "            Word_Emotion_dict[\"doveish\"].append(key)\n",
    "        elif doveish_score / len(doveish_word_list) == hawkish_score / len(hawkish_word_list):\n",
    "            print(f\"{key} 해당 키는 중립적입니다\")\n",
    "        elif doveish_score / len(doveish_word_list) < hawkish_score / len(hawkish_word_list):\n",
    "            #print(f\"{key} 해당 키는 매파적입니다\")\n",
    "            Word_Emotion_dict[\"hawkish\"].append(key)\n",
    "\n",
    "    print(\"학습 단어 극성 분류 사전 생성 하였습니다\")\n",
    "    print(\"새로운 문서에 대한 스코어 계산을 시작합니다\")\n",
    "\n",
    "    # 새로운 문서에 대한 스코어 계산\n",
    "    new_word_doveish_score = 0\n",
    "    new_word_hawkish_score = 0\n",
    "    for token in target_text:\n",
    "        if token in Word_Emotion_dict['doveish']:\n",
    "            new_word_doveish_score += 1 \n",
    "        elif token in Word_Emotion_dict['hawkish']:\n",
    "            new_word_hawkish_score += 1\n",
    "    print(new_word_doveish_score, new_word_hawkish_score)\n",
    "    if new_word_doveish_score >  new_word_hawkish_score:\n",
    "        return f\"{target_date} 해당 날짜는 비둘기적입니다\"\n",
    "    elif new_word_doveish_score ==  new_word_hawkish_score:\n",
    "        return f\"{target_date} 해당 날짜는 중립 성향 입니다\"\n",
    "    elif new_word_doveish_score <  new_word_hawkish_score:\n",
    "        return f\"{target_date} 해당 날짜는 매파적입니다\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과값\n",
    "model_start(target_date,x_tain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정확도 계산\n",
    "# 콜금리기준으로 맞춘 스코어와 못맞춘 스코어를 계산합니다.\n",
    "\n",
    "#1 극성 페이지 추출\n",
    "def make_Emotion_dict():\n",
    "    f = open(f\"./call_rate_months.csv\", 'r')\n",
    "    file_lines = f.readlines()\n",
    "    date = file_lines[0].split(\",\")\n",
    "    rate = file_lines[1].split(\",\")\n",
    "    date = date[:-4]\n",
    "    rate = rate[:-4]\n",
    "    date_dict = {date[idx]: rate[idx] for idx in range(len(rate))}\n",
    "    target_date_dict = {}\n",
    "    for i in date_dict:\n",
    "        temp = i.replace(\"/\",\".\")\n",
    "        target_date_dict[temp] = date_dict[i]\n",
    "    key_list = [key for key, value in target_date_dict.items()]\n",
    "    value_list = [value for key, value in target_date_dict.items()]\n",
    "    Emotion_dict = {\"doveish\":[], \"hawkish\":[]}\n",
    "    for idx in range(len(key_list)-1):\n",
    "        target_date = key_list[idx]\n",
    "        target_rate = value_list[idx]\n",
    "        compare_date = key_list[idx+1]\n",
    "        compare_rate = value_list[idx+1]\n",
    "        threshold_temp = float(target_rate) - float(compare_rate)\n",
    "        threshold_temp = (threshold_temp**2)**0.5\n",
    "        if threshold_temp > 0.001:\n",
    "            if target_rate > compare_rate:\n",
    "                Emotion_dict[\"doveish\"] += [[target_date, compare_date]]\n",
    "            else:\n",
    "                Emotion_dict[\"hawkish\"] += [[target_date, compare_date]]\n",
    "    return Emotion_dict\n",
    "\n",
    "Emotion_dict = make_Emotion_dict()\n",
    "Emotion_dict\n",
    "\n",
    "MPB_dict = {\"doveish\":[], \"hawkish\":[]}\n",
    "for Emotions in Emotion_dict:\n",
    "    if Emotions == \"doveish\":\n",
    "        Emotion = list(Emotion_dict[Emotions])\n",
    "        for date in Emotion:\n",
    "            start = date[0]\n",
    "            end = date[1]\n",
    "            start_time = datetime.strptime(start, \"%Y.%m\")\n",
    "            end_time_1 = datetime.strptime(end, \"%Y.%m\")\n",
    "            end_time_2 = end_time_1 - timedelta(days=1)\n",
    "            for day in range((end_time_2-start_time).days+1):\n",
    "                target = (start_time + timedelta(days=day)).strftime(\"%Y-%m-%d\")\n",
    "                MPB_Path = f\"./MPB_crawling_dataset/{target}.txt\"\n",
    "                file_name = f\"{target}.txt\"\n",
    "                if os.path.isfile(MPB_Path):\n",
    "                     MPB_dict[\"doveish\"].append(file_name)\n",
    "                     \n",
    "    else:\n",
    "        Emotion = list(Emotion_dict[Emotions])\n",
    "        for date in Emotion:\n",
    "            start = date[0]\n",
    "            end = date[1]\n",
    "            start_time = datetime.strptime(start, \"%Y.%m\")\n",
    "            end_time_1 = datetime.strptime(end, \"%Y.%m\")\n",
    "            end_time_2 = end_time_1 - timedelta(days=1)\n",
    "            for day in range((end_time_2-start_time).days+1):\n",
    "                target = (start_time + timedelta(days=day)).strftime(\"%Y-%m-%d\")\n",
    "                MPB_Path = f\"./MPB_crawling_dataset/{target}.txt\"\n",
    "                file_name = f\"{target}.txt\"\n",
    "                if os.path.isfile(MPB_Path):\n",
    "                    MPB_dict[\"hawkish\"].append(file_name)\n",
    "\n",
    "total_len = 0\n",
    "total = {\"answer\" : 0, \"wrong answer\" : 0}\n",
    "\n",
    "for i in MPB_dict['doveish']:\n",
    "    total_len += 1\n",
    "    target_MPB_Path = f\"./MPB_crawling_dataset/{i}\"\n",
    "    f = open(f\"{target_MPB_Path}\", 'r')\n",
    "    accuracy_text = f.readline()\n",
    "    accuracy_target = (phrases[accuracy_text.split()])\n",
    "    new_word_doveish_score = 0\n",
    "    new_word_hawkish_score = 0\n",
    "    for token in accuracy_target:\n",
    "        if token in Word_Emotion_dict['doveish']:\n",
    "            new_word_doveish_score += 1 \n",
    "        elif token in Word_Emotion_dict['hawkish']:\n",
    "            new_word_hawkish_score += 1\n",
    "    print(f\"비둘기 점수: {new_word_doveish_score}, 매파 점수: {new_word_hawkish_score}\")\n",
    "    if new_word_doveish_score >  new_word_hawkish_score:\n",
    "        total[\"answer\"] += 1\n",
    "    else:\n",
    "        total[\"wrong answer\"] += 1\n",
    "\n",
    "for i in MPB_dict[\"hawkish\"]:\n",
    "    total_len += 1\n",
    "    target_MPB_Path = f\"./MPB_crawling_dataset/{i}\"\n",
    "    f = open(f\"{target_MPB_Path}\", 'r')\n",
    "    accuracy_text = f.readline()\n",
    "    accuracy_target = (phrases[accuracy_text.split()])\n",
    "    new_word_doveish_score = 0\n",
    "    new_word_hawkish_score = 0\n",
    "    for token in accuracy_target:\n",
    "        if token in Word_Emotion_dict['doveish']:\n",
    "            new_word_doveish_score += 1 \n",
    "        elif token in Word_Emotion_dict['hawkish']:\n",
    "            new_word_hawkish_score += 1\n",
    "    print(f\"비둘기 점수: {new_word_doveish_score}, 매파 점수: {new_word_hawkish_score}\")\n",
    "    if new_word_doveish_score <  new_word_hawkish_score:\n",
    "        total[\"answer\"] += 1\n",
    "    else:\n",
    "        total[\"wrong answer\"] += 1\n",
    "    if new_word_doveish_score >  new_word_hawkish_score:\n",
    "        total[\"answer\"] += 1\n",
    "\n",
    "print(f\"정답률 : {total['answer'] / total_len}, 오답률 :{total['wrong answer'] / total_len}\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tone = (hawkish_count - dovish_count) / (hawkish_count + dovish_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_and_policy_rate_df.corr(method='pearson')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
